{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.5 Perplejidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Perplejidad:** $\\;$  **impredecibilidad** de un distribución de probabilidad discreta $p$\n",
    "$$\\operatorname{perplexity}(p)=2^{\\mathbb{H}(p)}$$\n",
    "\n",
    "**Impredecibilidad mínima:** $\\;$ se alcanza con una distribución de entropía nula, por lo que la perplejidad es uno\n",
    "$$\\operatorname{perplexity}(p)=2^0=1$$\n",
    "\n",
    "**Impredecibilidad máxima:** $\\;$ se da cuando $p$ es uniforme, con entropía $\\log_2 K$ (sobre $K$ estados), por lo que la perplejidad es $K$\n",
    "$$\\operatorname{perplexity}(p)=2^{\\log_2 K}=K$$\n",
    "\n",
    "**Perplejidad conjunta con la empírica:** $\\;$ dada una distribución $p$, podemos medir cómo de bien predice $\\mathcal{D}$ con\n",
    "$$\\operatorname{perplexity}(p_{\\mathcal{D}},p)=2^{\\mathbb{H}(p_{\\mathcal{D}},p)}\\qquad\\text{con}\\quad%\n",
    "p_{\\mathcal{D}}(x\\mid\\mathcal{D})=\\frac{1}{N}\\sum_{n=1}^N\\delta_{x_n}(x)$$\n",
    "\n",
    "**Ejemplo:** $\\;$ si $\\mathcal{D}$ es un único documento $x$ de longitud $N$ y $p$ una **unigrama,** la entropía cruzada y perplejidad son\n",
    "$$\\begin{align*}\n",
    "\\mathbb{H}(p_{\\mathcal{D}},p)&=-\\frac{1}{N}\\sum_{n=1}^N\\log_2p(x_n)\\\\\n",
    "\\operatorname{perplexity}(p_{\\mathcal{D}},p)%\n",
    "&=2^{\\mathbb{H}(p_{\\mathcal{D}},p)}=\\sqrt[N]{\\prod_n\\frac{1}{p(x_n)}}\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
