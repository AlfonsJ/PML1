{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropía conjunta\n",
    "\n",
    "**Entropía conjunta:** $\\;$ de dos variables aleatorias $X$ e $Y$ es\n",
    "$$\\mathbb{H}(X,Y)=-\\sum_{x,y}p(x,y)\\log_2p(x,y)$$\n",
    "\n",
    "**Ejemplo:** $\\;X(n)=1$ si $n$ es par, $Y(n)=1$ si $n$ es primo, $n$ de $1$ a $8$\n",
    "\n",
    "<div><table border-collapse: collapse><tr>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=400>\n",
    "\n",
    "$$\\begin{array}{ccccccccc}\n",
    "n&1&2&3&4&5&6&7&8\\\\\\hline%\n",
    "X&0&1&0&1&0&1&0&1\\\\%\n",
    "Y&0&1&1&0&1&0&1&0%\n",
    "\\end{array}$$\n",
    "\n",
    "</td>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=400>\n",
    "\n",
    "$$\\begin{array}{c|cc|c}\n",
    "p(X,Y)&Y=0&Y=1&p(Y)\\\\\\hline%\n",
    "X=0&1/8&3/8&1/2\\\\%\n",
    "X=1&3/8&1/8&1/2\\\\\\hline%\n",
    "p(X)&1/2&1/2%\n",
    "\\end{array}$$\n",
    "\n",
    "</td>\n",
    "<td style=\"border: none; text-align:left; vertical-align:top; padding:0; margin:0;\" width=400>\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{H}(X,Y)%\n",
    "&=-2\\frac{1}{8}\\log_2\\frac{1}{8}-2\\frac{3}{8}\\log_2\\frac{3}{8}\\\\[2mm]%\n",
    "&=-0.25\\,(-3)-0.75\\,(-1.4150)\\\\%\n",
    "&=1.81 ~\\text{bits}%\n",
    "\\end{align*}$$\n",
    "\n",
    "</td></tr></table></div>\n",
    "\n",
    "**Cota superior:** $\\;$ suma de las entropías marginales (la igualdad se cumple si $X$ e $Y$ son independientes)\n",
    "$$\\mathbb{H}(X,Y)\\leq \\mathbb{H}(X)+\\mathbb{H}(Y)$$\n",
    "\n",
    "**Ejemplo (cont.):** $\\;1.81 = \\mathbb{H}(X,Y)\\leq \\mathbb{H}(X)+\\mathbb{H}(Y) = 2$\n",
    "\n",
    "**Cota inferior:** $\\;$ si $X$ determina $Y$, $\\mathbb{H}(X,Y)=\\mathbb{H}(X)$, por lo que\n",
    "$$\\mathbb{H}(X,Y)\\geq\\max(\\mathbb{H}(X),\\mathbb{H}(Y))\\geq 0$$\n",
    "\n",
    "**Interpretación:** $\\;$ la combinación de variables no reduce la entropía; esto es, debemos observar más datos (no más variables) para reducir la incertidumbre\n",
    "\n",
    "**Extensión a más variables:** $\\;$ inmediata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
