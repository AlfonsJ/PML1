{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1.4 Optimización suave vs no-suave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización suave\n",
    "\n",
    "**Optimización suave:** $\\;$ objetivo y restricciones son funciones continuamente diferenciables, esto es, con derivadas parciales continuas\n",
    "\n",
    "**Constante de Lipschitz:** $\\;$ mide el grado de suavidad de una función (continuamente diferenciable)\n",
    "\n",
    "**Constante de Lipschitz en 1d:** $\\;$ cualquier (la menor) $\\;L\\geq 0\\;$ tal que $\\;\\lvert f(x_1)-f(x_2)\\rvert\\leq L\\lvert x_1-x_2\\rvert\\;$ para todo $\\;x_1, x_2\\in\\mathbb{R}$\n",
    "\n",
    "**Interpretación en 1d:** $\\;$ si $f$ es Lipschitz continua con $L>0$, existe un cono doble cuyo origen se mueve a lo largo de $f$ de manera que la gráfica completa siempre queda fuera del cono; esto es, $L$ es una pendiente que acota linealmente variaciones de $f$ en valor absoluto\n",
    "<div align=\"center\"><img src=\"Figure_8.8.png\" width=\"400\"/></div>\n",
    "\n",
    "**Importancia de la constante de Lipschitz en ML:** $\\;$ Se sabe que $L$ juega un papel crucial en el comportamiento de modelos (funciones) usuales en aprendizaje automático como las redes neuronales. Por ejemplo, si un modelo varía drásticamente frente a un cambio pequeño de la entrada, es de esperar que se muestre vulnerable frente a ejemplos adversariales y que no generalice bien con datos futuros. En caso opuesto, si el modelo varía extremadamente poco, es de esperar que exhiba un elevado sesgo y, en definitiva, que resulte poco útil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización no-suave\n",
    "\n",
    "**Optimización no-suave:** $\\;$ el gradiente del objetivo o restricciones no está bien definido en algunos puntos al menos\n",
    "\n",
    "**Objetivo compuesto:** $\\;\\mathcal{L}=\\mathcal{L}_s+\\mathcal{L}_r\\;$ tal que $\\mathcal{L}_s$ contiene términos suaves, diferenciables, mientras que $\\mathcal{L}_r$ recoge términos no suaves\n",
    "\n",
    "**Objetivos compuestos en ML:** $\\;\\mathcal{L}_s$ pérdida empírica suave y $\\mathcal{L}_r$ regularizador no suave (p.e. la norma $\\ell_1$ de los parámetros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
