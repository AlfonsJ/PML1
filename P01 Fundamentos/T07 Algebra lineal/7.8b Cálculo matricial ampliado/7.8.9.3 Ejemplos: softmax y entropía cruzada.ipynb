{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.8.9.3 Ejemplos: softmax y entropía cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropía cruzada tras una softmax\n",
    "\n",
    "Toma logits $\\boldsymbol{a}\\in\\mathbb{R}^{C}$ y etiquetas (one-hot) $\\boldsymbol{y}\\in\\{0,1\\}^C$; devuelve un escalar\n",
    "$$\\mathcal{L}=\\operatorname{CrossEntropy}(\\boldsymbol{y},\\boldsymbol{a})=-\\sum_cy_c\\log \\hat{y}_c%\n",
    "\\qquad\\text{con}\\qquad%\n",
    "\\hat{y}_c=\\mathcal{S}(\\boldsymbol{a})_c=\\dfrac{\\exp(a_c)}{\\sum_{c'}\\exp(a_{c'})}$$\n",
    "\n",
    "Si $\\,\\displaystyle\\boldsymbol{y}=\\operatorname{one-hot}(c),\\,$ podemos expresarla más sencillamente\n",
    "$$\\mathcal{L}=-\\log(\\hat{y}_c)=-\\log\\biggl[\\frac{\\exp(a_c)}{\\sum_j\\exp(a_j)}\\biggr]=\\log\\biggl[\\sum\\nolimits_j\\exp(a_j)\\biggr]-a_c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente de la pérdida con respecto a los logits\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{a}}=\\begin{bmatrix}\\dfrac{\\partial\\mathcal{L}}{\\partial a_1}&\\cdots&\\dfrac{\\partial\\mathcal{L}}{\\partial a_C}\\end{bmatrix}=(\\hat{\\boldsymbol{y}}-\\boldsymbol{y})^t\\in\\mathbb{R}^{1\\times C}$$\n",
    "ya que, para todo $i$,\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial a_i}=\\frac{\\partial}{\\partial a_i}\\log\\sum_j\\exp(a_j)-\\frac{\\partial}{\\partial a_i}a_c=\\frac{\\exp(a_i)}{\\sum_j\\exp(a_j)}-\\mathbb{I}(i=c)=\\hat{y}_i-\\mathbb{I}(i=c)$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
